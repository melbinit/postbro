# OpenAI Integration Plan

## Overview
Add OpenAI GPT-4o support alongside existing Gemini service, with environment variable to switch between models.

---

## Current Architecture

### Current Flow:
1. `analysis/tasks.py` â†’ calls `analyze_post_with_gemini()` from `analysis/services/gemini_service.py`
2. `gemini_service.py` â†’ loads prompt template, builds prompts, calls Gemini API
3. Returns analysis result as JSON

### What We Need:
- Similar service for OpenAI (`analysis/services/openai_service.py`)
- Environment variable to select model (`USE_MODEL=openai` or `USE_MODEL=gemini`)
- Update `tasks.py` to use selected model
- Both services use same prompt template

---

## Implementation Plan

### Phase 1: Create OpenAI Service
**File:** `postbro_backend/analysis/services/openai_service.py`

**What it does:**
- Mirrors `gemini_service.py` structure
- Uses same prompt template (`prompts/v1.txt`)
- Calls OpenAI GPT-4o API
- Enables input caching (50% discount on system prompt)
- Returns same JSON structure as Gemini

**Key Features:**
- âœ… Uses `get_system_prompt()` and `build_user_prompt()` (shared functions)
- âœ… Implements retry logic for rate limits
- âœ… Logs API calls to analytics
- âœ… Handles JSON parsing and validation
- âœ… Enables `cache_control={"type": "ephemeral"}` for input caching

**Function Signature:**
```python
def analyze_post_with_openai(
    platform: str,
    task_id: str,
    post_data: Dict,
    media_urls: List[str],
    video_length: Optional[int] = None,
    transcript: Optional[str] = None,
    frames: Optional[List[str]] = None,
    user_id: Optional[str] = None,
) -> Dict:
```

---

### Phase 2: Create Shared Prompt Utilities
**File:** `postbro_backend/analysis/services/prompt_utils.py` (NEW)

**Why:** Both Gemini and OpenAI need the same prompt building logic. Extract to shared module.

**Functions:**
- `load_prompt_template()` - Load v1.txt
- `get_system_prompt()` - Extract system prompt section
- `build_user_prompt()` - Build user prompt with post data

**Benefits:**
- âœ… DRY (Don't Repeat Yourself)
- âœ… Single source of truth for prompts
- âœ… Easy to update prompts for both models

---

### Phase 3: Update Gemini Service
**File:** `postbro_backend/analysis/services/gemini_service.py`

**Changes:**
- Import shared prompt utilities from `prompt_utils.py`
- Remove duplicate `load_prompt_template()`, `get_system_prompt()`, `build_user_prompt()`
- Keep Gemini-specific logic (API calls, response parsing)

---

### Phase 4: Add Model Selection Logic
**File:** `postbro_backend/analysis/services/__init__.py` (NEW) or update existing

**Function:**
```python
def get_analysis_service():
    """
    Returns the appropriate analysis service based on USE_MODEL env var.
    
    Returns:
        Function: analyze_post_with_gemini or analyze_post_with_openai
    """
    use_model = os.getenv('USE_MODEL', 'gemini').lower()
    
    if use_model == 'openai':
        from .openai_service import analyze_post_with_openai
        return analyze_post_with_openai
    else:
        from .gemini_service import analyze_post_with_gemini
        return analyze_post_with_gemini
```

---

### Phase 5: Update Celery Task
**File:** `postbro_backend/analysis/tasks.py`

**Changes:**
- Replace direct import: `from analysis.services.gemini_service import analyze_post_with_gemini`
- Use dynamic import: `from analysis.services import get_analysis_service`
- Call: `analyze_post = get_analysis_service()` then `analyze_post(...)`

**Location:** Around line 652-704 where Gemini is called

---

### Phase 6: Environment Variables
**File:** `.env`

**Add:**
```bash
# Model Selection: 'openai' or 'gemini' (default: gemini)
USE_MODEL=openai

# OpenAI API Key
OPENAI_API_KEY=sk-proj-...

# Gemini API Key (keep existing)
GEMINI_API_KEY_1=...
```

---

### Phase 7: Dependencies
**File:** `postbro_backend/requirements.txt`

**Add:**
```
openai>=1.0.0
```

---

## File Structure After Implementation

```
postbro_backend/
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py          # get_analysis_service() function
â”‚   â”‚   â”œâ”€â”€ prompt_utils.py      # Shared prompt building (NEW)
â”‚   â”‚   â”œâ”€â”€ gemini_service.py    # Updated (uses prompt_utils)
â”‚   â”‚   â””â”€â”€ openai_service.py    # NEW - OpenAI service
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â””â”€â”€ v1.txt                # Same prompt template (unchanged)
â”‚   â””â”€â”€ tasks.py                  # Updated (uses get_analysis_service)
```

---

## Implementation Details

### OpenAI Service Structure

```python
# analysis/services/openai_service.py

from openai import OpenAI
from .prompt_utils import get_system_prompt, build_user_prompt
from analytics.tasks import log_external_api_call

client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

def analyze_post_with_openai(...):
    # 1. Build prompts (using shared functions)
    system_prompt = get_system_prompt()
    user_prompt = build_user_prompt(...)
    
    # 2. Call OpenAI API with caching
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},  # Gets cached!
            {"role": "user", "content": user_prompt}
        ],
        cache_control={"type": "ephemeral"},  # Enable input caching
        response_format={"type": "json_object"},  # Force JSON
        temperature=0.4,
    )
    
    # 3. Parse response (same JSON structure as Gemini)
    # 4. Log to analytics
    # 5. Return analysis result
```

### Key Differences: Gemini vs OpenAI

| Feature | Gemini | OpenAI |
|---------|--------|--------|
| **Client** | `genai.GenerativeModel()` | `OpenAI().chat.completions.create()` |
| **System Prompt** | `system_instruction` parameter | `messages[0]` with `role: "system"` |
| **JSON Format** | `response_mime_type="application/json"` | `response_format={"type": "json_object"}` |
| **Caching** | Not available | `cache_control={"type": "ephemeral"}` |
| **Response** | `response.text` | `response.choices[0].message.content` |
| **Usage Info** | `response.usage_metadata` | `response.usage` |

---

## Testing Plan

### 1. Unit Tests
- Test prompt building (shared utilities)
- Test OpenAI service with mock responses
- Test model selection logic

### 2. Integration Tests
- Test full analysis flow with OpenAI
- Test fallback to Gemini if OpenAI fails
- Test environment variable switching

### 3. Manual Testing
- âœ… Set `USE_MODEL=openai` â†’ Process a post â†’ Verify OpenAI is used
- âœ… Set `USE_MODEL=gemini` â†’ Process a post â†’ Verify Gemini is used
- âœ… Check analytics logs show correct service
- âœ… Verify JSON response structure matches Gemini

---

## Error Handling

### OpenAI-Specific Errors:
- **401 Unauthorized** â†’ Invalid API key
- **429 Rate Limit** â†’ Retry with exponential backoff
- **500 Server Error** â†’ Retry (up to 3 times)
- **Invalid JSON** â†’ Log error, provide defaults

### Fallback Strategy:
- If OpenAI fails and `USE_MODEL=openai`, log error and raise
- Consider adding fallback to Gemini if OpenAI fails (optional)

---

## Cost Considerations

### With Input Caching Enabled:
- **System prompt** (3,000 tokens) â†’ $1.25/1M = $0.00375 (cached)
- **User prompt** (1,500 tokens) â†’ $2.50/1M = $0.00375 (not cached)
- **Output** (2,400 tokens) â†’ $10/1M = $0.024
- **Total per post: ~$0.0315** (with caching)

### Without Caching:
- **Total per post: ~$0.054**

**Savings with caching: ~42% on input costs**

---

## Migration Steps

1. âœ… Create `prompt_utils.py` with shared functions
2. âœ… Update `gemini_service.py` to use shared utilities
3. âœ… Create `openai_service.py` using shared utilities
4. âœ… Create `services/__init__.py` with `get_analysis_service()`
5. âœ… Update `tasks.py` to use `get_analysis_service()`
6. âœ… Add `openai` to `requirements.txt`
7. âœ… Add environment variables to `.env`
8. âœ… Test with both models
9. âœ… Update documentation

---

## Environment Variable Options

### Option 1: Simple String
```bash
USE_MODEL=openai  # or 'gemini'
```

### Option 2: With Default
```python
USE_MODEL = os.getenv('USE_MODEL', 'gemini').lower()
```

**Recommendation:** Option 2 (defaults to Gemini for backward compatibility)

---

## Backward Compatibility

- âœ… Default to Gemini if `USE_MODEL` not set
- âœ… Keep all existing Gemini code working
- âœ… No breaking changes to API or database
- âœ… Existing analyses continue to work

---

## Next Steps After Implementation

1. **Monitor Costs:**
   - Track OpenAI usage in analytics
   - Compare costs vs Gemini
   - Optimize prompt size if needed

2. **A/B Testing (Optional):**
   - Test quality differences between models
   - Compare response times
   - User feedback on analysis quality

3. **Advanced Features:**
   - Per-user model selection (premium feature?)
   - Automatic fallback if one model fails
   - Model performance metrics

---

## Estimated Time

- **Phase 1 (OpenAI Service):** 2-3 hours
- **Phase 2 (Shared Utils):** 30 minutes
- **Phase 3 (Update Gemini):** 30 minutes
- **Phase 4 (Model Selection):** 30 minutes
- **Phase 5 (Update Tasks):** 30 minutes
- **Phase 6-7 (Config):** 15 minutes
- **Testing:** 1-2 hours

**Total: ~5-7 hours**

---

## Approval Checklist

- [ ] Review plan and architecture
- [ ] Confirm environment variable name (`USE_MODEL`)
- [ ] Confirm OpenAI model (`gpt-4o`)
- [ ] Confirm caching strategy (ephemeral)
- [ ] Approve to proceed with implementation

---

*Ready to implement once approved!* ðŸš€






